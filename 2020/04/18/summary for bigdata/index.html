<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>summary for bigdata | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="sql题select    t1.date,    t1.qq,    t1.msg,    t1.活跃度,    t2.regionfrom (    select        date,        qq,msg,        case when msg&gt;200 then ‘高活跃’ when msg &lt;200 and msg &gt;0 then ‘低活跃’ else ‘未">
<meta property="og:type" content="article">
<meta property="og:title" content="summary for bigdata">
<meta property="og:url" content="http://yoursite.com/2020/04/18/summary%20for%20bigdata/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="sql题select    t1.date,    t1.qq,    t1.msg,    t1.活跃度,    t2.regionfrom (    select        date,        qq,msg,        case when msg&gt;200 then ‘高活跃’ when msg &lt;200 and msg &gt;0 then ‘低活跃’ else ‘未">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/04/18/summary%20for%20bigdata/RCFile%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://yoursite.com/2020/04/18/summary%20for%20bigdata/hive%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE.png">
<meta property="og:image" content="http://yoursite.com/2020/04/18/summary%20for%20bigdata/arrange.png">
<meta property="og:image" content="http://yoursite.com/2020/04/18/summary%20for%20bigdata/spark.png">
<meta property="article:published_time" content="2020-04-18T15:15:47.691Z">
<meta property="article:modified_time" content="2020-05-11T13:46:46.577Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/04/18/summary%20for%20bigdata/RCFile%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-summary for bigdata" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/18/summary%20for%20bigdata/" class="article-date">
  <time datetime="2020-04-18T15:15:47.691Z" itemprop="datePublished">2020-04-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      summary for bigdata
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="sql题"><a href="#sql题" class="headerlink" title="sql题"></a>sql题</h1><p>select<br>    t1.date,<br>    t1.qq,<br>    t1.msg,<br>    t1.活跃度,<br>    t2.region<br>from (<br>    select<br>        date,<br>        qq,msg,<br>        case when msg&gt;200 then ‘高活跃’ when msg &lt;200 and msg &gt;0 then ‘低活跃’ else ‘未知’ end as ‘活跃度’<br>    from table_act<br>    where DATE_FORMAT(date,’%Y%m’) = ‘201701’) t1 , (<br>    select<br>        date,<br>        qq,<br>        region<br>        from table_user<br>        where<br>        gender = ‘女’ and<br>        age &gt;= 18 and<br>        region = ‘广东省’<br>        ) t2<br>        where t1.qq = t2.qq<br>    and t1.date = t2.date;</p>
<p>– where timestimpdiff(day,date,’201701’)=0</p>
<h1 id="hive数据倾斜如何处理？"><a href="#hive数据倾斜如何处理？" class="headerlink" title="hive数据倾斜如何处理？"></a>hive数据倾斜如何处理？</h1><p>1.增加reduce的JVM内存<br>    适用于单值有大量记录的情况，但只记录超过reduce内存，无论怎么分区都不会有改变。<br>2.增加reduce的个数<br>    唯一值比较多，单个为一值不超过reduce内存，可以增加reduce数量，可以缓解某些reduce分了较多记录的情况<br>3.customer partition<br>    比如hadoop definitive guide里面的温度问题，一个固定的组合（测试站点的位置和温度）的分布是固定的，对于特定的查询如果前面两种方式都没有作用，自己实现partition也许可以解决问题。<br>4.其他优化讨论<br>    重新定义Key<br>    调参：<br>        hive.map.aggr=true// 开启map端聚合功能<br>        hive.groupby.skewindata=true// 开启两个MR job 第一个map的输出结果集会随机分配到reduce端，每个reduce做部分聚合<br>        map side join // 直接在map端就完成表的join操作，进入map端的数据根据分片得到，数据比较均衡（小表小于25M）<br>        通过以下方法来在map执行前合并小文件，减少map数：<br>        set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat<br>5.reduce sort merge 排序算法<br>    map reduce 一旦发生数据倾斜partition就失效了，对于join例子，某一个key分配了过多的数据。使用map-reduce-reduce方法，同一个key不需要分配到同一个reduce中，第一个reduce得到结果，第二个reduce进行汇总去重。<br>6.hive skewed join<br>    大表中的join key能够分散，对于同样join key的小表中的<br>    又能匹配到所有大表中的记录。<br>7.pipeline<br>    在join的时候可以直接使用group操作符减少大量的磁盘IO，而不是等待join完成，然后写入磁盘，group又读取磁盘做group操作，<br>8.distinct<br>    count(distinct x),<br>    ①可以把reduce个数调大<br>    ②可以把hive.exec.reducers.byte.per.reducer调小<br>    ③使用where子句先过滤掉一些数据<br>9.index,bitmap index<br>    index即为物化视图，对于group by 和distinct 的情况等变成了map端在做计算自然不存在数据倾斜。</p>
<p>精简：<br>1.增加JVM内存 （单值多超JVM内存）<br>2.增加Reducer个数 （单值多不超JVM内存）<br>3.customer partition<br>    比如hadoop definitive guide里面的温度问题，一个固定的组合（测试站点的位置和温度）<br>4.参数调优<br>    hive.map.aggr=true// 开启map端聚合功能<br>    hive.groupby.skewindata=true// 开启两个MR job 第一个map的输出结果集会随机分配到reduce端，每个reduce做部分聚合<br>    map side join // 直接在map端就完成表的join操作，进入map端的数据根据分片得到，数据比较均衡（小表小于25M）<br>5.sql调优<br>    1.使用物化视图，index,bitmap index<br>    2.先使用where子句减少数据量</p>
<h1 id="线程和进程的区别"><a href="#线程和进程的区别" class="headerlink" title="线程和进程的区别"></a>线程和进程的区别</h1><p>根本区别：进程是操作系统资源分配的基本单位，而线程是任务调度和执行的基本单位。</p>
<p>开销方面：每个进程都有独立的代码和数据空间，程序之间切换开销大；线程可以看作轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换开销小。</p>
<p>所处环境：在操作系统中能同时运行多个进程；而再同一个进程中可以有多个线程（通过CPU调度，每个时间片中只能一个线程执行）</p>
<p>内存分配方面：系统运行的时候会为每个进程分配不同的内存空间；而线程，除了CPU外，系统不会为线程分配内存（线程所使用的资源来自其所属进程资源），线程组之间只能共享资源。</p>
<p>包含关系：没有现成的进程可以看做是单线程，如果一个进程内有多个线程，则执行过程不是一条线的，而是多条线共同完成；线程是进程的一部分，所以线程也被称为轻权进程或者轻量级进程；</p>
<h1 id="int表示多少范围"><a href="#int表示多少范围" class="headerlink" title="int表示多少范围"></a>int表示多少范围</h1><p>int有4个字节，每个字节八位二进制，即32位二进制，又因最高位位符号位0正，1负，所以-2^31 ~ 2^32-1次<br>（byte 1 short char 2 int float 4 double long 8）</p>
<h1 id="认证授权outh2"><a href="#认证授权outh2" class="headerlink" title="认证授权outh2"></a>认证授权outh2</h1><p>密码模式（resource owner password credentials）(为遗留系统设计)(支持refresh token)</p>
<p>授权码模式（authorization code）(正宗方式)(支持refresh token)</p>
<p>简化模式（implicit）(为web浏览器应用设计)(不支持refresh token)</p>
<p>客户端模式（client credentials）(为后台api服务消费者设计)(不支持refresh token)</p>
<p>偏向锁，轻量锁，重量锁特点？死锁怎么产生？如何处理？<br>自旋：<br>    自旋是指某线程需要获取锁，但该锁已经被其他线程占用时，该线程不会被挂起，而是在不断的消耗CPU的时间，不停的试图获取锁。</p>
<p>偏向锁<br>    引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令。<br>    当只有一个线程去竞争锁的时候，我们不需要阻塞，也不需要自旋，因为只有一个线程在竞争，我们只要去判断该偏向锁中的ThreadID是否为当前线程即可。如果是就执行同步代码，不是就尝试使用CAS修改ThreadID，修改成功执行同步代码，不成功就将偏向锁升级成轻量锁。</p>
<p>轻量锁<br>    获取轻量锁的过程与偏向锁不同，竞争锁的线程首先需要拷贝对象头中的Mark Word到帧栈的锁记录中。拷贝成功后使用CAS操作尝试将对象的Mark Word更新为指向当前线程的指针。如果这个更新动作成功了，那么这个线程就拥有了该对象的锁。如果更新失败，那么意味着有多个线程在竞争。<br>    当竞争线程尝试占用轻量级锁失败多次之后（使用自旋）轻量级锁就会膨胀为重量级锁，重量级线程指针指向竞争线程，竞争线程也会阻塞，等待轻量级线程释放锁后唤醒他。</p>
<p>重量锁<br>    重量级锁的加锁、解锁过程和轻量级锁差不多，区别是：竞争失败后，线程阻塞，释放锁后，唤醒阻塞的线程，不使用自旋锁，不会那么消耗CPU，所以重量级锁适合用在同步块执行时间长的情况下。</p>
<p>死锁由于锁之间嵌套造成的,四个必要条件<br>    1.互斥使用，一个资源被一个线程使用时，别的线程不能使用<br>    2.不可抢占，资源请求者不能从资源占有者手中抢夺资源<br>    3.请求和保持，当资源请求者再请求其他的资源的同时，保持对原有资源的占有。<br>    4.循环等待，即存在一个等待队列，P1占有P2的资源，P2占有P1的资源<br>解决办法:<br>    破坏‘请求和保持’：<br>    1.所有进程在运行之前，必须一次性地申请在整个运行过程中所需的全部资源。<br>    2.要求每个进程提出新的资源申请前，释放它所占有的资源。<br>    破坏：‘不可抢占’<br>    如果一个进程请求当前被另一个进程占有的一个资源，则操作系统可以抢占另一个进程，要求它释放资源。只有在任意两个进程的优先级都不相同的条件下，该方法才能预防死锁。<br>    破坏“循环等待”条件：<br>    将系统中的所有资源统一编号，进程可在任何时刻提出资源申请，但所有申请必须按照资源的编号顺序（升序）提出。这样做就能保证系统不出现死锁。</p>
<h1 id="怎么查看集群性能？"><a href="#怎么查看集群性能？" class="headerlink" title="怎么查看集群性能？"></a>怎么查看集群性能？</h1><p>w<br>22:02:07 up 10 min,  1 user,  load average: 0.00, 0.07, 0.09<br>命令结果中load average 三个参数表示1 5 15 分钟系统的平均负载<br>cat /proc/cpuinfo prossor 0 表示cpu核数1与负载数对比判断是否过载；<br>top<br>按1，则可以展示出服务器有多少CPU，及每个CPU的使用情况<br>iostat -x 1 10<br>1表示时间 10表示次数 rsec/s表示读入，wsec/s表示每秒写入 util表示IO使用率</p>
<h1 id="行存储和列存储的区别？"><a href="#行存储和列存储的区别？" class="headerlink" title="行存储和列存储的区别？"></a>行存储和列存储的区别？</h1><p>1）行存储的写入是一次性完成，消耗的时间比列存储少，并且能够保证数据的完整性，缺点是数据读取过程中会产生冗余数据，如果只有少量数据，此影响可以忽略;数量大可能会影响到数据的处理效率。<br>2）列存储在写入效率、保证数据完整性上都不如行存储，它的优势是在读取过程，不会产生冗余数据，这对数据完整性要求不高的大数据处理领域，比如互联网，犹为重要。</p>
<p>HBase工作原理？有哪些节点？<br>HBase Table 组成：Table = RowKey + Family + Column + Timestamp + value</p>
<p>HBase中的一个Table按照行键分割为多个Hregion<br>每个Region中按列族划分为多个Hstore<br>每个Hstore中有一个memStore和多个HFile</p>
<h1 id="hive文件的存储格式有哪些？"><a href="#hive文件的存储格式有哪些？" class="headerlink" title="hive文件的存储格式有哪些？"></a>hive文件的存储格式有哪些？</h1><p>①textfile:默认格式，导入数据时会直接把数据文件拷贝到hdfs上，不进行处理。<br>②sequencefile:一种Hadoop API提供的二进制文件，使用方便、可分割，可压缩（压缩格式NONE，RECODE,BLOCK(常用)）等特点。<br>配置方法：<br>    SET hive.exec.compress.output=true<br>    SET io.seqfile.compresstion.type=BLOCK<br>    create table test2(str String) stored as sequenencefile;<br>③RCFile结合行存储查询速度快，和列存储节约空间的特点设计<br>Description:先按行划分为多个RowGroup（默认4M），然后垂直划分列式存储（横向列式存储）；<br>按列使用Gzip压缩，查询的时候按列维度进行解压（lazy解压机制，符合条件列的列才会被解压）<br><img src="./RCFile%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.png" alt=""><br>    1）同一行的数据位于同一节点，因此元组重构的开销很低。<br>    2）块内列存储，可以进行列维度的数据压缩，跳过不必要的行读取。<br>        实际过程：在map阶段从远端拷贝任然是整个数据块到本地目录，通过扫描每一个row group的头部定义来实现。<br>④ORCFile：hive给出的新格式，属于RCFile的升级版<br>        ⑴每个Task只输出单个文件，可以减少NameNode的负载；<br>        ⑵数据格式：支持datetime.decimal,以及一些复杂类型(struct,list,map，union类型)；<br>        Date<br>        ⑶文件中存储了一些轻量级的索引数据<br>        ⑷用多个互相独立的RecordReaders并行的读取相同的文件；<br>        ⑸支持行级别的更新和删除，实现事务表机制。<br>        ⑹针对不同的数据类型，采用不用的压缩算法进行优化。数值类型（游程编码），字符类型（字典编码）<br>⑤Parquet File:</p>
<h1 id="etl相关工具"><a href="#etl相关工具" class="headerlink" title="etl相关工具"></a>etl相关工具</h1><p>ETL 工具<br>    ①datestage<br>    ②informatica<br>    ③kettle<br>    ④ODI<br>    ⑤cognos<br>    ⑥beeload</p>
<p>调度工具<br>    ①Control-M<br>    ②taskctl<br>    ③ets<br>    ④moia</p>
<h1 id="hive-拉链表"><a href="#hive-拉链表" class="headerlink" title="hive 拉链表"></a>hive 拉链表</h1><p>定义：记录一个事物从开始，一直到当前的状态的所有变化信息。<br>区分流水表：在流水表中会存放一个用户的每条记录，但是在拉链表中只有一条记录<br> 拉链表增量操作sql:<br> INSERT OVERWRITE TABLE dws.user_his<br>SELECT * FROM<br>(<br>    SELECT A.user_num,<br>           A.mobile,<br>           A.reg_date,<br>           A.t_start_time,<br>           CASE<br>                WHEN A.t_end_time = ‘9999-12-31’ AND B.user_num IS NOT NULL THEN ‘2017-01-01’<br>                ELSE A.t_end_time<br>           END AS t_end_time<br>    FROM dws.user_his AS A<br>    LEFT JOIN ods.user_update AS B<br>    ON A.user_num = B.user_num<br>UNION<br>    SELECT C.user_num,<br>           C.mobile,<br>           C.reg_date,<br>           ‘2017-01-02’ AS t_start_time,<br>           ‘9999-12-31’ AS t_end_time<br>    FROM ods.user_update AS C<br>) AS T;</p>
<h1 id="hive-数据清洗常用函数？"><a href="#hive-数据清洗常用函数？" class="headerlink" title="hive 数据清洗常用函数？"></a>hive 数据清洗常用函数？</h1><p>①case when condition_clause then field end as ‘field’ // if(sc.score&gt;=60,1,null)<br>②regexp_replace(field,’/t’,’’)<br>③split(filed,’/t’)<br>④concat(‘1’,’2’)<br>⑤trim<br>⑥nvl(field,0) // ifnull(filed,0) </p>
<p>select name,count(bb.order_by_brand) from (select name,row_number() over (partition by name order by id desc) as order_by_brand from b)bb group by name; </p>
<h1 id="数据库分层"><a href="#数据库分层" class="headerlink" title="数据库分层"></a>数据库分层</h1><p>ODS Operational Data Store(原始数据)<br>DWD Data Warehouse Detail(数据明细层) 去除空值，脏数据，拥有详细的明细数据<br>DWS Data Warehouse Service(服务数据层) 轻度聚合：结合业务粒度<br>ADS Application Data Store(实时个性化维度汇总层) 做分析处理同步到RDS数据库<br>DIM(维度层)</p>
<h1 id="shell-脚本日期处理"><a href="#shell-脚本日期处理" class="headerlink" title="shell 脚本日期处理"></a>shell 脚本日期处理</h1><p>表示当前日期的前一天<br>date -d “-1 day” +%F<br>2020-04-05</p>
<h1 id="sql调优"><a href="#sql调优" class="headerlink" title="sql调优"></a>sql调优</h1><p>避免全表扫描<br>1.给常用的条件列建索引<br>2.不要使用Null值判断，!= 、&lt;&gt;<br>3.where子句中 使用or来连接条件，条件字段有索引<br>（优化改用Union all）<br>4.慎用in、not in<br>5.使用exists 代替in<br>6.避免在字段中使用操作符<br>7.使用复合索引时，尽量让字段顺序与索引顺序一致<br>8.对于数据量大的表，先分页再join<br>9.索引提高了读性能，同时降低了写的性能，每次都要重建索引<br>10.使用char代替varchar</p>
<p>精简：<br>1.增加JVM内存 （单值多超JVM内存）<br>2.增加Reducer个数/减小每个reduce处理的数据量 （单值多不超JVM内存）<br>3.customer partition<br>    比如hadoop definitive guide里面的温度问题，一个固定的组合（测试站点的位置和温度）<br>4.参数调优<br>    hive.map.aggr=true// 开启map端聚合功能<br>    hive.groupby.skewindata=true// 开启两个MR job 第一个map的输出结果集会随机分配到reduce端，每个reduce做部分聚合，第二MR再根据group by key分布到reduce中完成最终的聚合；<br>    map side join // 直接在map端就完成表的join操作，进入map端的数据根据分片得到，数据比较均衡（小表小于25M）<br>5.sql调优<br>    1.使用物化视图，index,bitmap index<br>    2.先使用where子句减少数据量</p>
<h1 id="外部表和内部标的区别"><a href="#外部表和内部标的区别" class="headerlink" title="外部表和内部标的区别"></a>外部表和内部标的区别</h1><p>1）创建时：创建内部表时，会将数据移动到数据仓库指定的路径；创建外部表时，不会对数据的位置进行改变。<br>2）删除表时：内部表元数据和数据被一起删除；而外部表只删除元数据，不删除数据</p>
<h1 id="hql执行流程图"><a href="#hql执行流程图" class="headerlink" title="hql执行流程图"></a>hql执行流程图</h1><p><img src="./hive%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt=""><br>1.向客户端提交一条HQL语句<br>2.获取执行计划，通过Complier进行编译，知道要操作的表<br>3.获取元数据，去找到要操作表的信息<br>4.拿到信息<br>5.编译器提交HQL语句分析方案<br>6.执行HQL语句方案<br>执行器在执行方案时，会判断如果当前方案不涉及到MR组件，比如为表添加分区信息、比如字符串操作等，比如简单的查询操作等，此时就会直接和元数据库交互，然后去HDFS上去找具体数据。<br>如果方案需要转换成MR job，则会将job 提交给Hadoop的JobTracker。<br>7.MR执行完任务告知执行引擎，执行引擎与HDFS交互获取结果<br>8.将结果返回给客户端，并展示在UI界面</p>
<h1 id="hive中group-by-和-count-distinct-哪个好"><a href="#hive中group-by-和-count-distinct-哪个好" class="headerlink" title="hive中group by 和 count(distinct)哪个好"></a>hive中group by 和 count(distinct)哪个好</h1><p>group by更好<br>原因：group by在map端会使用combine进行聚合，而count(distinct)不会，造成reduce的压力比较大。</p>
<h1 id="创建外部表-以表中的年龄字段创建分区表"><a href="#创建外部表-以表中的年龄字段创建分区表" class="headerlink" title="创建外部表 以表中的年龄字段创建分区表"></a>创建外部表 以表中的年龄字段创建分区表</h1><p>create external table stu(id int,name string,gender string,age int) row format delimited fields terminated by ‘ ‘;<br>load data inpath ‘hdfs://hadoop01:9000/stu.txt’ into table stu;<br>[load data local inpath ‘/home/stu.txt’ into table stu;]</p>
<p>–表中已有分区字段<br>create external table stu(id int,name string,gender string,age int) row format delimited fields terminated by ‘ ‘;<br>load data local inpath ‘/home/stu.txt’ into table stu;<br>create table p_stu1(id int,name string,gender string) partitioned by(age int) row format delimited fields terminated by ‘ ‘;<br>insert into table p_stu1 partition(age) select id,name,gender,age from stu distribute by age;</p>
<p>– 表中无分区字段<br>create table stud1(id int,name string,gender string) partitioned by(age int) row format delimited fields terminated by ‘ ‘;<br>load data local inpath ‘/home/15/stu1.txt’ into table stud1 partition(age=15);</p>
<p>– 分桶表<br>create table tmp(id int,name string,gender string) clustered by(id) into 6 buckets row format delimited fields terminated by ‘ ‘;<br>load data local inpath ‘/home/15/stu1.txt’ into table tmp;</p>
<p>sort by 排序 distribute 分组 order by 全局排序</p>
<p>– 包含map类型的写法<br>create table infos(id int,info map&lt;string,int&gt;) row format delimited fields terminated by ‘ ‘ map keys terminated by ‘,’;<br>load data local inpath ‘/home/mapstu.txt’ into table infos;<br>– struct<br>create table struct(id int,s struct<a href="name:string,age:int,gender:string">name:string,age:int,gender:string</a>) row format delimited fields terminated by ‘ ‘ collection items terminated by ‘,’;</p>
<p>– serde<br>提取前：192.168.120.23 – [30/Apr/2018:20:25:34 +0800] “GET /music.mp3 HTTP/1.1” 304 -<br>提取后：192.168.120.23  30/Apr/2018:20:25:34    +0800   GET     /bg.css HTTP/1.1 304<br>create table log(ip string,time string,zone string,requesttype string,url string,protocol string,status int) row format serde ‘org.apache.hadoop.hive.serde2.RegexSerDe’ with serdeproperties(“input.regex”=”(.<em>) -- \[(.</em>) (.<em>)\] &quot;(.</em>) (.<em>) (.</em>)&quot; (.*) -“) stored as textfile;<br>load data local inpath ‘/home/serde.txt’ into table log;</p>
<p>create table log_orc(ip string,time string,zone string,requesttype string,url string,protocol string) partitioned by (status int) row format delimited fields terminated by ‘ ‘ stored as orc;<br>insert into log_orc partition(status) select * from log distribute by status;</p>
<p>– left semi join a表中的数据哪些在b中出现过<br>select * from product_t left semi  join order_t on product_t.pid=order_t.pid;<br>a表里哪些数据在b表中出现过</p>
<p>– hql 实现wordcount<br>select tmp.word,count(1) as cnt from<br>(select explode(split(line,’ ‘)) as word from wordcount1)tmp<br>group by tmp.word<br>order by cnt desc;</p>
<h1 id="留存率-每日活跃用户hql语句写法"><a href="#留存率-每日活跃用户hql语句写法" class="headerlink" title="留存率 每日活跃用户hql语句写法"></a>留存率 每日活跃用户hql语句写法</h1><p>– 留存率计算<br>     select  dim_date<br>        ,node_id<br>        ,total_cnt<br>        ,concat_ws(‘% | ‘, cast(round(dif_1cnt<em>100/total_cnt, 2) as string), cast(dif_1cnt as string))<br>        ,concat_ws(‘% | ‘, cast(round(dif_2cnt</em>100/total_cnt, 2) as string), cast(dif_2cnt as string))<br>        ,concat_ws(‘% | ‘, cast(round(dif_3cnt<em>100/total_cnt, 2) as string), cast(dif_3cnt as string))<br>        ,concat_ws(‘% | ‘, cast(round(dif_4cnt</em>100/total_cnt, 2) as string), cast(dif_4cnt as string))<br>    from<br>    (<br>        select p1.state dim_date<br>            ,p1.node_id<br>            ,count(distinct p1.user_id) total_cnt<br>            ,count(distinct if(datediff(p3.state,p1.state) = 1, p1.user_id, null)) dif_1cnt<br>            ,count(distinct if(datediff(p3.state,p1.state) = 2, p1.user_id, null)) dif_2cnt<br>            ,count(distinct if(datediff(p3.state,p1.state) = 3, p1.user_id, null)) dif_3cnt<br>            ,count(distinct if(datediff(p3.state,p1.state) = 4, p1.user_id, null)) dif_4cnt<br>        from<br>            (<br>                select<br>                    from_unixtime(unix_timestamp(cast(partition_date as string), ‘yyyyMMdd’), ‘yyyy-MM-dd’) state,<br>                    user_id，<br>                    node_id<br>                from user_active_day<br>                where partition_date between date1 and date2<br>                and user_is_new = 1<br>                group by 1,2,3  –如果不行用字段替换，group by去重优于distinct<br>            )p1 –日新增用户名单(register_date,user_id)<br>            left outer join<br>            (<br>                select<br>                    from_unixtime(unix_timestamp(cast(partition_date as string), ‘yyyyMMdd’), ‘yyyy-MM-dd’) state,<br>                    user_id,<br>                    node_id<br>                from active_users<br>                where partition_date between date1 and date2<br>                group by 1,2,3<br>            )p3 –期间活跃用户(active_date,user_id)<br>            on (p3.user_id = p1.user_id and p3.node_id=p1.node_id)<br>        group by 1,2<br>    ) p4;</p>
<h1 id="1-sql中where-子句中-and-or-的执行顺序-2-left-join情况下on-后面where和and区别"><a href="#1-sql中where-子句中-and-or-的执行顺序-2-left-join情况下on-后面where和and区别" class="headerlink" title="1)sql中where 子句中 and or ()的执行顺序 2)left join情况下on 后面where和and区别"></a>1)sql中where 子句中 and or ()的执行顺序 2)left join情况下on 后面where和and区别</h1><p>and or ()执行顺序<br>如下sql说出执行结果并解释原因：<br>select * from stu where sid=1 and sgender=’男’ or sname=’aaa’;<br>select * from stu where sid=1 and (sgender=’男’ or sname=’aaa’);<br>第一条查询出id为1并且性别为男或者者名字为aaa的学生信息;<br>第二条查询出id为1并且为男或者id为1并且名字为aaa的学生。</p>
<p>在left,right,full join中使用on条件进行关联时，and,where子条件会使查询的结果不同，使用and才会查询出left等表中所有数据，使用where时跟inner join一样只返回关联上的结果集。</p>
<h1 id="增量-全量hql-以及行转置sql"><a href="#增量-全量hql-以及行转置sql" class="headerlink" title="增量 全量hql 以及行转置sql"></a>增量 全量hql 以及行转置sql</h1><h1 id="手写sqoop倒数据脚本"><a href="#手写sqoop倒数据脚本" class="headerlink" title="手写sqoop倒数据脚本"></a>手写sqoop倒数据脚本</h1><p>bin/sqoop import –connect jdbc:mysql://hadoop01:3306/test –username root –password root –table tabx –target-dir ‘/sqoop/tabx’ –fields-terminated-by ‘|’ -m -1;<br>// 按id进行分区–split-by id<br>insert overwirte directory ‘/sqoop/tabx/s.txt’ select F1,F2,F3 from taby stored as textfile<br>bin/sqoop export –connect jdbc:mysql://hadoop01://3306/test –username root –password root –table taby –export-dir ‘/sqoop/tabx/s.txt’ –fields-terminated-by ‘ ‘ –columns F1,F2,F3 –update-key F4 –update-mode allowinsert<br>bin/sqoop export –connect jdbc:mysql://hadoop01:3306/test –username root –password root –table taby –export-dir ‘/sqoop/tabx/part-m-00000’ –fields-terminated-by ‘|’ -m -1</p>
<h1 id="spark-常用操作-以及调优"><a href="#spark-常用操作-以及调优" class="headerlink" title="spark 常用操作 以及调优"></a>spark 常用操作 以及调优</h1><p>val r1 = sc.textFile(“file:///home/1.txt”)<br>val r2 = sc.flatMap{<em>.split(‘ ‘)}.map{(</em>,1)}.reduceByKey{<em>+</em>}</p>
<p>1.将spark的序列化由Java原生序列化更换为kryo<br>2.配置临时文件目录，将每个目录路径挂载到不同的磁盘上<br>3.启动推测执行机制<br>4.生产环境，查看运行结果不要使用collect或者foreach，建议使用savaAsTextFile,存储后查看<br>5.特定场景下，使用MapPartitions代替Map</p>
<h1 id="sql进行行列转换"><a href="#sql进行行列转换" class="headerlink" title="sql进行行列转换"></a>sql进行行列转换</h1><p>– 行转列<br>select name,case when subject=’chinese’ then score else 0 end as ‘chinese’ from s1 group by name;<br>– 列转行<br>select name,’chinese’,chinese as score from s2 group by name;</p>
<h1 id="kylin-练习"><a href="#kylin-练习" class="headerlink" title="kylin 练习"></a>kylin 练习</h1><p>执行数据模型，维度和度量<br>create table dept(dept int,dname string,location string) row format delimited fields terminated by ‘ ‘;<br>create table emp (emp int,ename string,job string,mgr double,hiredate date,salary double,common double,deptno int) row format delimited fields terminated by ‘ ‘;</p>
<p>10 Accounting new-york<br>20 research dallas<br>30 sales chicago<br>40 oprations boston</p>
<p>7369 SMITH CLERK 7902 1980-12-17 800 NULL 20<br>7499 ALLEN SALESMAN 7698 1981-02-20 1600 300 30<br>7521 WARD SALESMAN 7698 1981-02-22 1250 500 30<br>7566 JONES MANAGER 7839 1981-04-02 2975 NULL 20<br>7654 MARTIN SALESMAN 7698 1981-09-28 1250 1400 30</p>
<p>select job,sum(salary) from emp join dept on deptno=dept group by ename;</p>
<p>查看cube状态<br>bin/kylin.sh org.apache.kylin.engine.mr.common.CubeStatsReader Mp</p>
<p>– 减枝优化<br>强制维度<br>层级维度 比如 年月<br>联合维度 比如 张三 18</p>
<h1 id="复购用户sql-留存率"><a href="#复购用户sql-留存率" class="headerlink" title="复购用户sql 留存率"></a>复购用户sql 留存率</h1><p>select<br>    x3.fd,<br>    count(distinct x3.id) as ‘当日首购用户数’,<br>    count(distinct case when x3.cnt&gt;0 and x3.cnt&lt;30 then x3.id else null end) as ‘当月复购用户数’<br> from<br>(select<br>    x1.id,<br>    x1.fd,<br>    datediff(date_format(x2.time,’%Y-%m-%d’),x1.fd) as cnt<br>    from<br>(select id,min(date_format(time,’%Y-%m-%d’)) fd from user_a group by id)x1 inner join user_a x2 on x1.id = x2.id)x3 group by x3.fd; </p>
<p>select<br>    x4.fd,<br>    x4.total ‘新增数’,<br>    cnt1_diff<em>100/total ‘次留率’,<br>    cnt2_diff</em>100/total ‘二留率’<br>from<br>(select<br>    x3.fd,<br>    count(distinct x3.id) total,<br>    count(distinct if(cnt=1,x3.id,null)) cnt1_diff,<br>    count(distinct if(cnt=2,x3.id,null)) cnt2_diff<br>    from<br>(select x1.fd,x1.id,datediff(x2.dt,x1.fd) cnt from<br>(select id,min(dt) fd from a_z group by id) x1<br>inner join<br>b_q x2 on x1.id = x2.id) x3 group by x3.fd) x4;</p>
<p>mysql 存储过程制造数据<br>delimiter $<br>drop procedure if exists t_add;<br>create procedure t_add(num int)<br>begin<br>declare i int;<br>set i = 1;<br>while i &lt; num do<br>insert into ua(id,dt) values(floor(50000+rand()*100),<br>date(<br>from_unixtime(<br>    unix_timestamp(‘2019-01-01’)<br>     + floor(<br>   rand() * ( unix_timestamp(‘2020-08-08’) - unix_timestamp(‘2019-01-01’) + 1 )<br> )<br>)));</p>
<p>set i=i+1;<br>end while;<br>end $</p>
<p>call t_add(1000000);</p>
<p>insert into result_ua<br>select u4.fd,u4.cnt5_diff/u4.total,u4.cnt10_100diff/u4.total,u4.cnt_100diff/u4.total from (select u3.fd,count(u3.id) as total,count(distinct case when u3.cnt&gt;0 and u3.cnt&lt;5 then id else null end) as cnt5_diff,count(distinct case when u3.cnt&gt;10 and u3.cnt&lt;100 then id else null end) as cnt10_100diff,count(distinct case when u3.cnt&gt;100 then id else null end) as cnt_100diff from(select u1.fd,u1.id,datediff(u2.dt,u1.fd) as cnt from(select id,min(dt) fd from ua group by id) u1 inner join ua u2 on u1.id=u2.id)u3 group by u3.fd) u4;</p>
<h1 id="spark-MLlib"><a href="#spark-MLlib" class="headerlink" title="spark MLlib"></a>spark MLlib</h1><p>‘’’<br>val conf = new SparkConf().setMaster(“spark://ip:7077”).setAppName(“als”);<br>val sc = new SparkContext(conf);<br>val data = sc.textFile(“f://data/a.txt”);<br>val movie = sc.textFile(“f://data/b.item”);<br>val movieMap = movie.map{line =&gt;<br>    val info = line.split(“\|”)<br>    val movieId = info(0).toInt<br>    val movieName = info(1)<br>    (movieId,movieName)<br>}.collectAsMap</p>
<p>val ratings =  data.map{<br>    line.split(“\t”)<br>     val userId = info(0).toInt<br>     val movieId = info(1).toInt<br>     val score = info(2).toInt<br>     Rating(userId,movieId,score)<br>}</p>
<p>val model = ALS.train(ratings,50,15,10)<br>val result = model.recommendProducts(789,10).map{<br>    rate =&gt;<br>        val userId = rate.user<br>         val movieId = rate.product<br>         val socre = rate.rating<br>         val movieName = movieMap.apply(movieId)<br>         (userId,movieName,socre)<br>}<br>//result.foreach(println)<br>model.save(sc,”hdfs://namenode:9000/aa”)<br>‘’’</p>
<p>– spark streaming<br>‘’’<br>val conf = new SparkConf().setMaster(“spark://ip:7077”).setAppName(“sparkStreaming”)<br>val ssc = new StreamingContext(new SparkConf(conf),Milliseconds(3000))<br>val data = ssc.textFileStream(“hdfs://namenode:9000/a.txt”)<br>val result = data.flatMap{<em>.split(“ “)}.map{(</em>,1)}.reduceByKey{<em>+</em>}<br>result.println<br>ssc.start<br>‘’’<br>ALS算法核心思想迭代多次求解一系列最小二乘回归问题，使得最终因子矩阵中的因子值是最优解（误差的平方和最小）</p>
<p>根据偏好矩阵，假定一个低阶的因子k，k小于维度m和n（m*n维）达到降维的目的。两个子因子矩阵相乘=原来的偏号矩阵。<br>推荐系统启动方式:</p>
<p>①用户冷启动 让用户选择兴趣爱好<br>②物品冷启动，根据算法 预估评分<br>③系统冷启动 购买数据 爬取数据</p>
<h1 id="mysql和Oracle-语法上的区别"><a href="#mysql和Oracle-语法上的区别" class="headerlink" title="mysql和Oracle 语法上的区别"></a>mysql和Oracle 语法上的区别</h1><p>SQL常识：聚集函数算窗口函数<br>常见的窗口函数：RowNumber() 123 RanK() 113 Dense_rank() 112<br>1.Mysql 中from 后面是子查询表，那么必须有别名<br>2.连接字符串Oracle || ,Mysql concat 方法<br>3.Mysql没有Oracle的动态游标，只有显示游标<br>4.Mysql的group by中可以是没有被select中的字段，Oracle中就会报错<br>5.执行顺序：From -&gt; Where -&gt; Group By子句 -&gt; Having子句 -&gt; Order By-&gt; Select -&gt; limit</p>
<h1 id="sqoop-如何保证数据一致性-遇到过哪些问题"><a href="#sqoop-如何保证数据一致性-遇到过哪些问题" class="headerlink" title="sqoop 如何保证数据一致性,遇到过哪些问题"></a>sqoop 如何保证数据一致性,遇到过哪些问题</h1><p>1.创建一个结构一样的 表名_temp的mysql表<br>2.使用如下两个参数控制，只有当数据全部成功导入临时表，才会再导入正真的表中<br>    –staging-table mysql表名_tmp <br>    –clear-staging-table \</p>
<h1 id="kettle使用"><a href="#kettle使用" class="headerlink" title="kettle使用"></a>kettle使用</h1><p>注意点：<br>    1.单表，直接sql做连接<br>    2.多源输入，用Database join 操作<br>    3.update table(单行执行)没有execute sql scrip速度块(多行执行)<br>    4.性能调优：<br>        ①使用数据库连接池<br>        ②尽量提高批处理commit size<br>        ③尽量使用缓存，缓存尽量大一些(主要是文本文件和数据流)<br>        ④JVM调优 调大参数，使-xms -xmx内存一样大，避免多次产生full GC<br>        ⑤可以使用sql来做的操作尽量用sql来做，group merge stream lookup split field这些比较慢<br>        ⑥插入大量数据的时候尽量删掉索引，避免update,可以先delete再insert<br>        ⑦删除整表数据使用truncate table 不要使用delete all row<br>        ⑧需要计算时，先用sql，处理不了就用储存过程，最后考虑calculate步骤<br><a href="https://blog.csdn.net/xingyue0422/article/details/86509995" target="_blank" rel="noopener">原文地址</a></p>
<h1 id="如何使用脚本抽取数据库表中是否有null值或者重复行？"><a href="#如何使用脚本抽取数据库表中是否有null值或者重复行？" class="headerlink" title="如何使用脚本抽取数据库表中是否有null值或者重复行？"></a>如何使用脚本抽取数据库表中是否有null值或者重复行？</h1><p><a href="https://www.php.cn/mysql-tutorials-128961.html" target="_blank" rel="noopener">JavaScript处理</a><br><a href="https://blog.csdn.net/zzq900503/article/details/78745359" target="_blank" rel="noopener">java处理</a></p>
<h1 id="数据库表分区"><a href="#数据库表分区" class="headerlink" title="数据库表分区"></a>数据库表分区</h1><p>1.一个表最多只能有1024个分区<br>2.如果分区字段中包含主键和所有索引，或者都不包含<br>3.分区表中无法使用外键约束<br>4.数据和索引会一起被分区<br>5.可以根据range、list、hash、key进行分区<br>创建分区表语法<br>create table table_name (columns int,…)<br>    partition by range(store_id)(<br>        partition p0 values less than (6),<br>        partition p1 values less than (11),…<br>        partition pn values less than MAXVALUE<br>    );</p>
<p>其他详情<br>partition by range columns(a,b) (partition p0 values less than (0,10),…);<br>partition by range(year(separated)) (partition p0 values less than (1996),…);<br>partition by list(store_id) (partition pNorth values in (3,5,6,9,17));<br>partition by hash(store_id) partitions 4;<br>partition by hash(year(hired) partitions 4;<br>partition by linear hash(year(hired) partitions 4;<br>partition by key(store_id);</p>
<h1 id="数据集市开发从需求沟通、标签定义、mapping规范、编码开发、测试验收、版本移交到生产验证整个流程"><a href="#数据集市开发从需求沟通、标签定义、mapping规范、编码开发、测试验收、版本移交到生产验证整个流程" class="headerlink" title="数据集市开发从需求沟通、标签定义、mapping规范、编码开发、测试验收、版本移交到生产验证整个流程"></a>数据集市开发从需求沟通、标签定义、mapping规范、编码开发、测试验收、版本移交到生产验证整个流程</h1><p><a href="https://www.cnblogs.com/dailidong/p/7571150.html" target="_blank" rel="noopener">answer</a><br><img src="arrange.png" alt="大数据整体结构图"><br>数据仓库是对多个异构的数据源有效集成，集成后按照主题进行了重组，并包含历史数据，而且存放在数据仓库中的数据一般不再修改<br>数据集市也叫数据市场，是一个从操作的数据和其他的为某个特殊专业人员团体服务的数据源中收集数据的仓库</p>
<h1 id="group-concat函数"><a href="#group-concat函数" class="headerlink" title="group_concat函数"></a>group_concat函数</h1><p>SQL:select id,GROUP_CONCAT(name,’’) new_name from tab1 group by id;<br>HQL:select id,concat_ws(‘,’,collect_set(name)) as new_name from tab1 group by id;</p>
<p>1    a,b<br>2    a,b</p>
<h1 id="spark-执行流程"><a href="#spark-执行流程" class="headerlink" title="spark 执行流程"></a>spark 执行流程</h1><p><img src="spark.png" alt="spark"></p>
<ul>
<li>1.构建Spark Application 的运行环境SparkContext，SparkContext向资源管理器(Standalone,Mesos或Yarn)注册并申请资源</li>
<li>2.资源管理器分配Executor资源并启动StandaloneExecutorBackend,Executor运行情况随着心跳发送到资源管理器上</li>
<li>3.SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task</li>
<li>4.Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。</li>
<li>5.Task在Executor上运行，运行完毕释放所有资源</li>
</ul>
<h1 id="持久化-和-CheckPoint区别"><a href="#持久化-和-CheckPoint区别" class="headerlink" title="持久化 和 CheckPoint区别"></a>持久化 和 CheckPoint区别</h1><ul>
<li>persist()可以将RDD的partition持久化到磁盘，但该partition由BlockManager管理,<br>一旦driver program结束，executor也会结束，BlockManager结束，被缓存到磁盘上的RDD也被清空</li>
<li>使用checkpoint将rdd持久化到hdfs或本地文件夹，任务结束不会被删除</li>
</ul>
<h1 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h1><ul>
<li>窄依赖：父RDD的每个分区只被一个子RDD使用　 一对一，多对一</li>
<li>宽依赖：父RDD的每个分区都可能被多子RDD使用 多对多，一对多</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/18/summary%20for%20bigdata/" data-id="ckautpojf000b94rhap72hjnd" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/04/19/mysql_review_note/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          mysql_review_note
        
      </div>
    </a>
  
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/05/31/spring%E8%A7%A3%E5%AF%86/">spring解密</a>
          </li>
        
          <li>
            <a href="/2020/05/15/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E8%89%BA%E6%9C%AF%E6%80%BB%E7%BB%93/">并发编程艺术总结</a>
          </li>
        
          <li>
            <a href="/2020/05/14/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">git常用命令</a>
          </li>
        
          <li>
            <a href="/2020/05/09/thread/">thread</a>
          </li>
        
          <li>
            <a href="/2020/05/09/netcompile/">netcompile</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>